# Fairness Definition Catalog — + Formulas

This file contains the core definitions of fairness in AI, explained in simple terms, with formulas and real-world examples.

---

## 1. Demographic Parity (Statistical Parity)

**Formula:**  
Mathematical form: P(Ŷ=1|A=a) = P(Ŷ=1|A=b) for all protected groups a, b. 
*Explanation:* The probability of being selected should be the same across all protected groups.

**Simple Explanation:**  
The system should select people from different demographic groups at the same rate, regardless of differences in qualifications or historical data.

**In practice:**  
- If 40% of selected applicants are Group A, roughly 40% should also be Group B.  
- Focuses on *equal representation*.

**When to use:**  
- Representation goals.  
- Historical exclusion exists.  
- Visibility/access matters as much as qualifications.

**Limitations:**  
- May select people with different qualification levels.  
- Does not guarantee “similar individuals” receive similar treatment.

**Example:**  
Leadership-development ads reach women and men in equal proportions.

---

## 2. Equal Opportunity

**Formula:**  
Mathematical form: P(Ŷ=1|Y=1,A=a) = P(Ŷ=1|Y=1,A=b) for all protected groups a, b.

**Simple Explanation:**  
Qualified individuals should have the same chance of selection, no matter the group.

**In practice:**  
- Focuses on truly qualified candidates.  
- Avoids missing qualified candidates from any group.

**When to use:**  
- False negatives are most harmful.  
- Ground-truth labels are reliable.

**Limitations:**  
- Does not control false positives.  
- Depends on possibly biased data.

**Example:**  
Medical tool identifies patients equally across demographic groups.

---

## 3. Equalized Odds

**Formula:**  
Mathematical form: P(Ŷ=1|Y=y,A=a) = P(Ŷ=1|Y=y,A=b) for y ∈ {0,1} and all protected groups a, b. 
*Explanation:* Both true positive and false positive rates should be equal across groups.

**Simple Explanation:**  
The system should balance errors across all demographic groups.

**In practice:**  
- Balances both false positives and false negatives.  
- Useful when both types of errors have consequences.

**When to use:**  
- High-stakes applications.  
- Fairness must cover all error types.

**Limitations:**  
- Complex modeling.  
- May reduce model accuracy.

**Example:**  
Credit-risk system avoids approving risky loans for one group more than another AND rejects safe borrowers equally.

---

## 4. Calibration / Sufficiency (Group Calibration)

**Formula:**  
\[
P(Y=1 \mid \hat{Y}=r, A=a) = P(Y=1 \mid \hat{Y}=r, A=b) \quad \forall a,b, \forall r
\]  
*Explanation:* A predicted score should have the same meaning across groups.

**Simple Explanation:**  
A risk or readiness score should be equally interpretable for every group.

**In practice:**  
- “0.7 score” implies the same likelihood of success for all groups.  
- Ensures comparability of scores.

**When to use:**  
- Probability outputs (credit, insurance, rankings).  
- Analysts need group-agnostic scores.

**Limitations:**  
- Can conflict with Demographic Parity.  
- Hard to achieve if base rates differ strongly.

**Example:**  
High repayment probability score means the same for every demographic group.
